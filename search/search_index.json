{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api/llm/","title":"LLM Module","text":""},{"location":"api/llm/#uglychain.llm","title":"<code>uglychain.llm</code>","text":""},{"location":"api/llm/#uglychain.llm.llm","title":"<code>llm(model, response_format=None, map_keys=None, **api_params)</code>","text":"<p>LLM \u88c5\u9970\u5668\uff0c\u7528\u4e8e\u6307\u5b9a\u8bed\u8a00\u6a21\u578b\u548c\u5176\u53c2\u6570\u3002</p> <p>:param model: \u6a21\u578b\u540d\u79f0 :param api_params: API \u53c2\u6570\uff0c\u4ee5\u5173\u952e\u5b57\u53c2\u6570\u5f62\u5f0f\u4f20\u5165 :return: \u8fd4\u56de\u4e00\u4e2a\u88c5\u9970\u5668\uff0c\u7528\u4e8e\u88c5\u9970\u63d0\u793a\u51fd\u6570</p> Source code in <code>src/uglychain/llm.py</code> <pre><code>def llm(\n    model: str,\n    response_format: type[T] | None = None,\n    map_keys: list[str] | None = None,\n    **api_params: Any,\n) -&gt; Callable[\n    [Callable[P, str | list[dict[str, str]] | T]],\n    Callable[P, str | list[str] | T | list[T] | ToolResopnse | list[ToolResopnse]],\n]:\n    \"\"\"\n    LLM \u88c5\u9970\u5668\uff0c\u7528\u4e8e\u6307\u5b9a\u8bed\u8a00\u6a21\u578b\u548c\u5176\u53c2\u6570\u3002\n\n    :param model: \u6a21\u578b\u540d\u79f0\n    :param api_params: API \u53c2\u6570\uff0c\u4ee5\u5173\u952e\u5b57\u53c2\u6570\u5f62\u5f0f\u4f20\u5165\n    :return: \u8fd4\u56de\u4e00\u4e2a\u88c5\u9970\u5668\uff0c\u7528\u4e8e\u88c5\u9970\u63d0\u793a\u51fd\u6570\n    \"\"\"\n    default_model_from_decorator = model\n    default_api_params_from_decorator = api_params.copy()\n\n    def parameterized_lm_decorator(\n        prompt: Callable[P, str | list[dict[str, str]] | T],\n    ) -&gt; Callable[P, str | list[str] | T | list[T] | ToolResopnse | list[ToolResopnse]]:\n        @wraps(prompt)\n        def model_call(\n            *prompt_args: P.args,\n            api_params: dict[str, Any] | None = None,  # type: ignore\n            **prompt_kwargs: P.kwargs,\n        ) -&gt; str | list[str] | T | list[T] | ToolResopnse | list[ToolResopnse]:\n            console = Console()\n            # \u83b7\u53d6\u88ab\u4fee\u9970\u51fd\u6570\u7684\u8fd4\u56de\u7c7b\u578b\n            response_model = ResponseModel[T](prompt, response_format)\n\n            # \u5408\u5e76\u88c5\u9970\u5668\u7ea7\u522b\u7684API\u53c2\u6570\u548c\u51fd\u6570\u7ea7\u522b\u7684API\u53c2\u6570\n            merged_api_params = config.default_api_params.copy()\n            if default_api_params_from_decorator:\n                merged_api_params.update(default_api_params_from_decorator)\n            if api_params:\n                merged_api_params.update(api_params)\n\n            # \u83b7\u53d6\u540c\u65f6\u8fd0\u884c\u7684\u6b21\u6570\n            n = merged_api_params.get(\"n\", 1)\n            # \u83b7\u53d6\u6a21\u578b\u540d\u79f0\n            model = merged_api_params.pop(\"model\", default_model_from_decorator)\n\n            console.log_model_usage_pre(model, prompt, prompt_args, prompt_kwargs)\n\n            m, map_args_index_set, map_kwargs_keys_set = _get_map_keys(prompt, prompt_args, prompt_kwargs, map_keys)\n            if m &gt; 1 and n &gt; 1:\n                raise ValueError(\"n &gt; 1 \u548c\u5217\u8868\u957f\u5ea6 &gt; 1 \u4e0d\u80fd\u540c\u65f6\u6210\u7acb\")\n\n            def process_single_prompt(i: int) -&gt; list[Any]:\n                args = [arg[i] if j in map_args_index_set else arg for j, arg in enumerate(prompt_args)]  # type: ignore\n                kwargs = {\n                    key: value[i] if key in map_kwargs_keys_set else value  # type: ignore\n                    for key, value in prompt_kwargs.items()\n                }\n                res = prompt(*args, **kwargs)  # type: ignore\n                assert (\n                    isinstance(res, str) or isinstance(res, list) and all(isinstance(item, dict) for item in res)\n                ), ValueError(\"\u88ab\u4fee\u9970\u7684\u51fd\u6570\u8fd4\u56de\u503c\u5fc5\u987b\u662f str \u6216 `messages`(list[dict[str, str]]) \u7c7b\u578b\")\n                messages = _get_messages(res, prompt)\n                response_model.process_parameters(model, messages, merged_api_params)\n\n                console.log_model_usage_post_info(messages, merged_api_params)\n\n                response = Client.generate(model, messages, **merged_api_params)\n\n                # \u4ece\u54cd\u5e94\u4e2d\u89e3\u6790\u7ed3\u679c\n                result = [response_model.parse_from_response(choice) for choice in response]\n                console.log_model_usage_post_intermediate(result)\n                return result\n\n            results = []\n            console.log_progress_start(m if m &gt; 1 else n)\n\n            if config.use_parallel_processing:\n                with ThreadPoolExecutor() as executor:\n                    futures = [executor.submit(process_single_prompt, i) for i in range(m)]\n\n                    for future in as_completed(futures):\n                        results.extend(future.result())\n            else:\n                for i in range(m):\n                    results.extend(process_single_prompt(i))\n\n            console.log_progress_end()\n\n            if len(results) == 0:\n                raise ValueError(\"\u6a21\u578b\u672a\u8fd4\u56de\u4efb\u4f55\u9009\u62e9\")\n            elif m == n == len(results) == 1:\n                return results[0]\n            return results\n\n        model_call.__api_params__ = default_api_params_from_decorator  # type: ignore\n        model_call.__func__ = prompt  # type: ignore\n\n        return model_call  # type: ignore\n\n    return parameterized_lm_decorator  # type: ignore[return-value]\n</code></pre>"},{"location":"api/react/","title":"ReAct Module","text":""},{"location":"api/react/#uglychain.react","title":"<code>uglychain.react</code>","text":""},{"location":"api/react/#uglychain.react.final_answer","title":"<code>final_answer(answer)</code>","text":"<p>When get Final Answer, use this tool to return the answer and finishes the task.</p> Source code in <code>src/uglychain/react.py</code> <pre><code>def final_answer(answer: str) -&gt; str:\n    \"\"\"When get Final Answer, use this tool to return the answer and finishes the task.\"\"\"\n    return answer\n</code></pre>"},{"location":"cn/","title":"UglyChain \u6587\u6863","text":"<p>\u6b22\u8fce\u6765\u5230 UglyChain \u6587\u6863\u4e2d\u5fc3\uff01</p>"},{"location":"cn/#_1","title":"\u5feb\u901f\u5bfc\u822a","text":"<ul> <li>\u5feb\u901f\u5f00\u59cb</li> <li>API \u53c2\u8003</li> <li>\u793a\u4f8b\u4ee3\u7801</li> <li>\u5f00\u53d1\u6307\u5357</li> </ul>"},{"location":"cn/#_2","title":"\u529f\u80fd\u7279\u6027","text":"<ul> <li>\u7b80\u6d01\u6613\u7528\u7684 API \u8bbe\u8ba1</li> <li>\u5f3a\u5927\u7684\u5de5\u5177\u94fe\u652f\u6301</li> <li>\u5b8c\u5584\u7684\u7c7b\u578b\u63d0\u793a</li> <li>\u4e30\u5bcc\u7684\u793a\u4f8b\u4ee3\u7801</li> </ul>"},{"location":"cn/development/","title":"Development Guide","text":""},{"location":"cn/development/#setup-development-environment","title":"Setup Development Environment","text":"<ol> <li> <p>Clone repository:    <pre><code>git clone https://github.com/uglychain/uglychain.git\ncd uglychain\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -r requirements-dev.txt\n</code></pre></p> </li> <li> <p>Configure pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"cn/development/#running-tests","title":"Running Tests","text":"<p>Run all tests: <pre><code>pytest tests/\n</code></pre></p> <p>Run specific test module: <pre><code>pytest tests/test_llm.py\n</code></pre></p> <p>Run with coverage report: <pre><code>pytest --cov=src tests/\n</code></pre></p>"},{"location":"cn/development/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Use black for code formatting</li> <li>Use isort for import sorting</li> <li>Use flake8 for linting</li> </ul>"},{"location":"cn/development/#contributing","title":"Contributing","text":"<ol> <li> <p>Create a new branch:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and commit:    <pre><code>git add .\ngit commit -m \"feat: your feature description\"\n</code></pre></p> </li> <li> <p>Push your branch:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a pull request on GitHub</p> </li> </ol>"},{"location":"cn/examples/","title":"Examples","text":""},{"location":"cn/examples/#basic-llm-usage","title":"Basic LLM Usage","text":"<pre><code>from uglychain import llm\n\n@llm(model=\"openai:gpt-4o-mini\", temperature=0.1)\ndef hello(world: str):\n    \"\"\"You are a helpful assistant that writes in lower case.\"\"\"\n    return f\"Say hello to {world[::-1]} with a poem.\"\n\nhello(\"sama\")\n</code></pre>"},{"location":"cn/examples/#structured-output","title":"Structured Output","text":"<pre><code>from pydantic import BaseModel\nfrom uglychain import llm\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@llm(\"openai:gpt-4o-mini\", response_format=UserDetail)\ndef test(name: str):\n    return f\"{name} is a boy\"\n\ntest(\"Bob\")\n</code></pre>"},{"location":"cn/examples/#mapchain-batch-processing","title":"MapChain (Batch Processing)","text":"<pre><code>@llm(\"openai:gpt-4o-mini\", map_keys=[\"input\"])\ndef map(input: list[str]):\n    return input\n\ninput = [\n    \"How old are you?\",\n    \"What is the meaning of life?\",\n    \"What is the hottest day of the year?\",\n]\nfor item in map(input):\n    print(item)\n</code></pre>"},{"location":"cn/examples/#reactchain-tool-usage","title":"ReActChain (Tool Usage)","text":"<pre><code>from uglychain import react\nfrom examples.utils import execute_command\n\n@react(\"openai:gpt-4o-mini\", tools=[execute_command])\ndef update():\n    return \"Update my computer system\"\n\nupdate()  # Automatically runs shell commands to update the system\n</code></pre>"},{"location":"cn/examples/#advanced-mapchain-example","title":"Advanced MapChain Example","text":"<p>```python class AUTHOR(BaseModel):     name: str = Field(..., description=\"Name\")     introduction: str = Field(..., description=\"Introduction\")</p> <p>@llm(\"openai:gpt-4o-mini\", map_keys=[\"book\"], response_format=AUTHOR) def map(book: list[str], position: str):     return f\"Who is the {position} of {book}?\"</p> <p>input = [     \"Dream of the Red Chamber\",     \"Journey to the West\",     \"Romance of the Three Kingdoms\",     \"Water Margin\", ] map(book=input, position=\"author\")  # Returns a list of AUTHOR objects</p>"},{"location":"cn/api/core/","title":"Core API Overview","text":""},{"location":"cn/api/core/#main-modules","title":"Main Modules","text":""},{"location":"cn/api/core/#llm-module","title":"LLM Module","text":"<ul> <li>Core functionality for interacting with language models</li> <li>Supports multiple model providers (OpenAI, Anthropic, etc.)</li> <li>Includes decorator-based interface for easy usage</li> </ul>"},{"location":"cn/api/core/#react-module","title":"ReAct Module","text":"<ul> <li>Implements reasoning and acting capabilities</li> <li>Supports tool usage and multi-step reasoning</li> <li>Built-in support for parallel processing</li> </ul>"},{"location":"cn/api/core/#structured-module","title":"Structured Module","text":"<ul> <li>Provides structured output capabilities</li> <li>Integrates with Pydantic models</li> <li>Supports automatic schema generation</li> </ul>"},{"location":"cn/api/core/#key-features","title":"Key Features","text":"<ul> <li>Decorator-based API: Simplify LLM interactions with intuitive decorators</li> <li>Structured Output: Easily parse LLM responses into structured data</li> <li>Parallel Processing: Process multiple inputs concurrently</li> <li>ReAct Support: Built-in support for reasoning and acting</li> <li>Extensible Architecture: Easily add custom models and tools</li> </ul>"},{"location":"cn/api/tools/","title":"Built-in Tools","text":""},{"location":"cn/api/tools/#execute_command","title":"execute_command","text":"<ul> <li>Executes shell commands</li> <li>Returns command output</li> <li>Example:   <pre><code>from uglychain.tools import execute_command\n\n@tool\ndef update_system():\n    return execute_command(\"sudo apt update &amp;&amp; sudo apt upgrade -y\")\n</code></pre></li> </ul>"},{"location":"cn/api/tools/#web_search","title":"web_search","text":"<ul> <li>Performs web searches</li> <li>Returns search results</li> <li>Example:   <pre><code>from uglychain.tools import web_search\n\n@tool\ndef search_weather(city: str):\n    return web_search(f\"Weather in {city}\")\n</code></pre></li> </ul>"},{"location":"cn/api/tools/#file_operations","title":"file_operations","text":"<ul> <li>Provides file system operations</li> <li>Includes read/write/delete functions</li> <li>Example:   <pre><code>from uglychain.tools import file_operations\n\n@tool\ndef create_file(path: str, content: str):\n    return file_operations.write(path, content)\n</code></pre></li> </ul>"},{"location":"cn/api/tools/#custom-tools","title":"Custom Tools","text":"<ul> <li>Create custom tools using @tool decorator</li> <li>Example:   ```python   from uglychain import tool</li> </ul> <p>@tool   def custom_tool(param: str):       return f\"Processed {param}\"</p>"},{"location":"cn/guide/configuration/","title":"Configuration Guide","text":""},{"location":"cn/guide/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"cn/guide/configuration/#required-variables","title":"Required Variables","text":"<ul> <li><code>OPENAI_API_KEY</code>: OpenAI API key</li> <li><code>ANTHROPIC_API_KEY</code>: Anthropic API key</li> <li>more can be found in aisuite</li> </ul>"},{"location":"cn/guide/configuration/#configuration-file","title":"Configuration File","text":"<p>not implemented yet</p>"},{"location":"cn/guide/configuration/#runtime-configuration","title":"Runtime Configuration","text":"<p>Override configuration at runtime:</p> <pre><code>from uglychain import config\nconfig.verbose = True # Enable verbose logging\nconfig.use_parallel_processing = True # Enable parallel processing\nconfig.show_progress = False # Disable progress bar\nconfig.default_api_params = {\n    \"temperature\": 0.1,\n    \"max_tokens\": 1000\n    # ... other default parameters\n}\n</code></pre>"},{"location":"cn/guide/quickstart/","title":"Quick Start","text":""},{"location":"cn/guide/quickstart/#installation","title":"Installation","text":"<p>Install using pdm:</p> <pre><code>pdm add uglychain\n</code></pre>"},{"location":"cn/guide/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"cn/guide/quickstart/#llm-decorator","title":"llm Decorator","text":"<pre><code>from uglychain import llm\n\n@llm(model=\"openai:gpt-4o-mini\", temperature=0.1)\ndef hello(world: str):\n    \"\"\"You are a helpful assistant that writes in lower case.\"\"\"\n    return f\"Say hello to {world[::-1]} with a poem.\"\n\nresponse = hello(\"sama\")\nprint(response)\n</code></pre>"},{"location":"cn/guide/quickstart/#structured-output","title":"Structured Output","text":"<pre><code>from pydantic import BaseModel\nfrom uglychain import llm\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@llm(model=\"openai:gpt-4o-mini\", response_format=UserDetail)\ndef parse_user(name: str):\n    return f\"{name} is a boy\"\n\nuser = parse_user(\"Bob\")\nprint(user)\n</code></pre>"},{"location":"cn/guide/quickstart/#mapchain-batch-processing","title":"MapChain (Batch Processing)","text":"<pre><code>@llm(model=\"openai:gpt-4o-mini\", map_keys=[\"input\"])\ndef batch_process(input: list[str]):\n    return input\n\ninputs = [\n    \"How old are you?\",\n    \"What is the meaning of life?\",\n    \"What is the hottest day of the year?\",\n]\n\nfor result in batch_process(inputs):\n    print(result)\n</code></pre>"},{"location":"cn/guide/quickstart/#reactchain-tool-usage","title":"ReActChain (Tool Usage)","text":"<pre><code>from uglychain import react\nfrom examples.utils import execute_command\n\n@react(model=\"openai:gpt-4o-mini\", tools=[execute_command])\ndef update_system():\n    return \"Update my computer system\"\n\nupdate_system()  # Automatically runs system update commands\n</code></pre>"},{"location":"cn/guide/quickstart/#configuration","title":"Configuration","text":"<p>The API Keys can be set as environment variables, or can be passed as config to the aisuite Client constructor. You can use tools like python-dotenv \u6216 direnv to set the environment variables manually.</p> <pre><code>export OPENAI_API_KEY=\"your-openai-api-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n</code></pre>"},{"location":"en/","title":"UglyChain Documentation","text":"<p>Welcome to UglyChain documentation center!</p>"},{"location":"en/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Quick Start</li> <li>API Reference</li> <li>Examples</li> <li>Development Guide</li> </ul>"},{"location":"en/#features","title":"Features","text":"<ul> <li>Clean and simple API design</li> <li>Powerful toolchain support</li> <li>Comprehensive type hints</li> <li>Rich example codebase</li> </ul>"},{"location":"en/development/","title":"Development Guide","text":""},{"location":"en/development/#setup-development-environment","title":"Setup Development Environment","text":"<ol> <li> <p>Clone repository:    <pre><code>git clone https://github.com/uglychain/uglychain.git\ncd uglychain\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -r requirements-dev.txt\n</code></pre></p> </li> <li> <p>Configure pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"en/development/#running-tests","title":"Running Tests","text":"<p>Run all tests: <pre><code>pytest tests/\n</code></pre></p> <p>Run specific test module: <pre><code>pytest tests/test_llm.py\n</code></pre></p> <p>Run with coverage report: <pre><code>pytest --cov=src tests/\n</code></pre></p>"},{"location":"en/development/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8 style guide</li> <li>Use black for code formatting</li> <li>Use isort for import sorting</li> <li>Use flake8 for linting</li> </ul>"},{"location":"en/development/#contributing","title":"Contributing","text":"<ol> <li> <p>Create a new branch:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes and commit:    <pre><code>git add .\ngit commit -m \"feat: your feature description\"\n</code></pre></p> </li> <li> <p>Push your branch:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a pull request on GitHub</p> </li> </ol>"},{"location":"en/examples/","title":"Examples","text":""},{"location":"en/examples/#basic-llm-usage","title":"Basic LLM Usage","text":"<pre><code>from uglychain import llm\n\n@llm(model=\"openai:gpt-4o-mini\", temperature=0.1)\ndef hello(world: str):\n    \"\"\"You are a helpful assistant that writes in lower case.\"\"\"\n    return f\"Say hello to {world[::-1]} with a poem.\"\n\nhello(\"sama\")\n</code></pre>"},{"location":"en/examples/#structured-output","title":"Structured Output","text":"<pre><code>from pydantic import BaseModel\nfrom uglychain import llm\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@llm(\"openai:gpt-4o-mini\", response_format=UserDetail)\ndef test(name: str):\n    return f\"{name} is a boy\"\n\ntest(\"Bob\")\n</code></pre>"},{"location":"en/examples/#mapchain-batch-processing","title":"MapChain (Batch Processing)","text":"<pre><code>@llm(\"openai:gpt-4o-mini\", map_keys=[\"input\"])\ndef map(input: list[str]):\n    return input\n\ninput = [\n    \"How old are you?\",\n    \"What is the meaning of life?\",\n    \"What is the hottest day of the year?\",\n]\nfor item in map(input):\n    print(item)\n</code></pre>"},{"location":"en/examples/#reactchain-tool-usage","title":"ReActChain (Tool Usage)","text":"<pre><code>from uglychain import react\nfrom examples.utils import execute_command\n\n@react(\"openai:gpt-4o-mini\", tools=[execute_command])\ndef update():\n    return \"Update my computer system\"\n\nupdate()  # Automatically runs shell commands to update the system\n</code></pre>"},{"location":"en/examples/#advanced-mapchain-example","title":"Advanced MapChain Example","text":"<p>```python class AUTHOR(BaseModel):     name: str = Field(..., description=\"Name\")     introduction: str = Field(..., description=\"Introduction\")</p> <p>@llm(\"openai:gpt-4o-mini\", map_keys=[\"book\"], response_format=AUTHOR) def map(book: list[str], position: str):     return f\"Who is the {position} of {book}?\"</p> <p>input = [     \"Dream of the Red Chamber\",     \"Journey to the West\",     \"Romance of the Three Kingdoms\",     \"Water Margin\", ] map(book=input, position=\"author\")  # Returns a list of AUTHOR objects</p>"},{"location":"en/api/core/","title":"Core API Overview","text":""},{"location":"en/api/core/#main-modules","title":"Main Modules","text":""},{"location":"en/api/core/#llm-module","title":"LLM Module","text":"<ul> <li>Core functionality for interacting with language models</li> <li>Supports multiple model providers (OpenAI, Anthropic, etc.)</li> <li>Includes decorator-based interface for easy usage</li> </ul>"},{"location":"en/api/core/#react-module","title":"ReAct Module","text":"<ul> <li>Implements reasoning and acting capabilities</li> <li>Supports tool usage and multi-step reasoning</li> <li>Built-in support for parallel processing</li> </ul>"},{"location":"en/api/core/#structured-module","title":"Structured Module","text":"<ul> <li>Provides structured output capabilities</li> <li>Integrates with Pydantic models</li> <li>Supports automatic schema generation</li> </ul>"},{"location":"en/api/core/#key-features","title":"Key Features","text":"<ul> <li>Decorator-based API: Simplify LLM interactions with intuitive decorators</li> <li>Structured Output: Easily parse LLM responses into structured data</li> <li>Parallel Processing: Process multiple inputs concurrently</li> <li>ReAct Support: Built-in support for reasoning and acting</li> <li>Extensible Architecture: Easily add custom models and tools</li> </ul>"},{"location":"en/api/tools/","title":"Built-in Tools","text":""},{"location":"en/api/tools/#execute_command","title":"execute_command","text":"<ul> <li>Executes shell commands</li> <li>Returns command output</li> <li>Example:   <pre><code>from uglychain.tools import execute_command\n\n@tool\ndef update_system():\n    return execute_command(\"sudo apt update &amp;&amp; sudo apt upgrade -y\")\n</code></pre></li> </ul>"},{"location":"en/api/tools/#web_search","title":"web_search","text":"<ul> <li>Performs web searches</li> <li>Returns search results</li> <li>Example:   <pre><code>from uglychain.tools import web_search\n\n@tool\ndef search_weather(city: str):\n    return web_search(f\"Weather in {city}\")\n</code></pre></li> </ul>"},{"location":"en/api/tools/#file_operations","title":"file_operations","text":"<ul> <li>Provides file system operations</li> <li>Includes read/write/delete functions</li> <li>Example:   <pre><code>from uglychain.tools import file_operations\n\n@tool\ndef create_file(path: str, content: str):\n    return file_operations.write(path, content)\n</code></pre></li> </ul>"},{"location":"en/api/tools/#custom-tools","title":"Custom Tools","text":"<ul> <li>Create custom tools using @tool decorator</li> <li>Example:   ```python   from uglychain import tool</li> </ul> <p>@tool   def custom_tool(param: str):       return f\"Processed {param}\"</p>"},{"location":"en/guide/configuration/","title":"Configuration Guide","text":""},{"location":"en/guide/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"en/guide/configuration/#required-variables","title":"Required Variables","text":"<ul> <li><code>OPENAI_API_KEY</code>: OpenAI API key</li> <li><code>ANTHROPIC_API_KEY</code>: Anthropic API key</li> <li>more can be found in aisuite</li> </ul>"},{"location":"en/guide/configuration/#configuration-file","title":"Configuration File","text":"<p>not implemented yet</p>"},{"location":"en/guide/configuration/#runtime-configuration","title":"Runtime Configuration","text":"<p>Override configuration at runtime:</p> <pre><code>from uglychain import config\nconfig.verbose = True # Enable verbose logging\nconfig.use_parallel_processing = True # Enable parallel processing\nconfig.show_progress = False # Disable progress bar\nconfig.default_api_params = {\n    \"temperature\": 0.1,\n    \"max_tokens\": 1000\n    # ... other default parameters\n}\n</code></pre>"},{"location":"en/guide/quickstart/","title":"Quick Start","text":""},{"location":"en/guide/quickstart/#installation","title":"Installation","text":"<p>Install using pdm:</p> <pre><code>pdm add uglychain\n</code></pre>"},{"location":"en/guide/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"en/guide/quickstart/#llm-decorator","title":"llm Decorator","text":"<pre><code>from uglychain import llm\n\n@llm(model=\"openai:gpt-4o-mini\", temperature=0.1)\ndef hello(world: str):\n    \"\"\"You are a helpful assistant that writes in lower case.\"\"\"\n    return f\"Say hello to {world[::-1]} with a poem.\"\n\nresponse = hello(\"sama\")\nprint(response)\n</code></pre>"},{"location":"en/guide/quickstart/#structured-output","title":"Structured Output","text":"<pre><code>from pydantic import BaseModel\nfrom uglychain import llm\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n@llm(model=\"openai:gpt-4o-mini\", response_format=UserDetail)\ndef parse_user(name: str):\n    return f\"{name} is a boy\"\n\nuser = parse_user(\"Bob\")\nprint(user)\n</code></pre>"},{"location":"en/guide/quickstart/#mapchain-batch-processing","title":"MapChain (Batch Processing)","text":"<pre><code>@llm(model=\"openai:gpt-4o-mini\", map_keys=[\"input\"])\ndef batch_process(input: list[str]):\n    return input\n\ninputs = [\n    \"How old are you?\",\n    \"What is the meaning of life?\",\n    \"What is the hottest day of the year?\",\n]\n\nfor result in batch_process(inputs):\n    print(result)\n</code></pre>"},{"location":"en/guide/quickstart/#reactchain-tool-usage","title":"ReActChain (Tool Usage)","text":"<pre><code>from uglychain import react\nfrom examples.utils import execute_command\n\n@react(model=\"openai:gpt-4o-mini\", tools=[execute_command])\ndef update_system():\n    return \"Update my computer system\"\n\nupdate_system()  # Automatically runs system update commands\n</code></pre>"},{"location":"en/guide/quickstart/#configuration","title":"Configuration","text":"<p>The API Keys can be set as environment variables, or can be passed as config to the aisuite Client constructor. You can use tools like python-dotenv \u6216 direnv to set the environment variables manually.</p> <pre><code>export OPENAI_API_KEY=\"your-openai-api-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n</code></pre>"}]}